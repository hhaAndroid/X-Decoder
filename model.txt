BaseModel(
  (model): GeneralizedXdecoder(
    (backbone): D2FocalNet(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): FocalModulationBlock(
              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=96, out_features=196, bias=True)
                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (drop_path): Identity()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): FocalModulationBlock(
              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=96, out_features=196, bias=True)
                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (drop_path): DropPath()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchEmbed(
            (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): FocalModulationBlock(
              (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=192, out_features=388, bias=True)
                (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (drop_path): DropPath()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): FocalModulationBlock(
              (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=192, out_features=388, bias=True)
                (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (drop_path): DropPath()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchEmbed(
            (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): FocalModulationBlock(
              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=384, out_features=772, bias=True)
                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): FocalModulationBlock(
              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=384, out_features=772, bias=True)
                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): FocalModulationBlock(
              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=384, out_features=772, bias=True)
                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): FocalModulationBlock(
              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=384, out_features=772, bias=True)
                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): FocalModulationBlock(
              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=384, out_features=772, bias=True)
                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): FocalModulationBlock(
              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=384, out_features=772, bias=True)
                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchEmbed(
            (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BasicLayer(
          (blocks): ModuleList(
            (0): FocalModulationBlock(
              (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=768, out_features=1540, bias=True)
                (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (drop_path): DropPath()
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): FocalModulationBlock(
              (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (modulation): FocalModulation(
                (f): Linear(in_features=768, out_features=1540, bias=True)
                (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
                (act): GELU()
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (focal_layers): ModuleList(
                  (0): Sequential(
                    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
                    (1): GELU()
                  )
                  (1): Sequential(
                    (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                    (1): GELU()
                  )
                  (2): Sequential(
                    (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)
                    (1): GELU()
                  )
                )
              )
              (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (drop_path): DropPath()
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (sem_seg_head): XdecoderHead(
      (pixel_decoder): TransformerEncoderPixelDecoder(
        (adapter_1): Conv2d(
          96, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
        (layer_1): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
        (adapter_2): Conv2d(
          192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
        (layer_2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
        (adapter_3): Conv2d(
          384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
        (layer_3): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
        (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (input_proj): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))
        (transformer): TransformerEncoderOnly(
          (encoder): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (1): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (2): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (3): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (4): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (5): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (pe_layer): Positional encoding PositionEmbeddingSine
            num_pos_feats: 256
            temperature: 10000
            normalize: True
            scale: 6.283185307179586
        (layer_4): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
      )
      (predictor): XDecoder(
        (pe_layer): Positional encoding PositionEmbeddingSine
            num_pos_feats: 256
            temperature: 10000
            normalize: True
            scale: 6.283185307179586
        (transformer_self_attention_layers): ModuleList(
          (0): SelfAttentionLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): SelfAttentionLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): SelfAttentionLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): SelfAttentionLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): SelfAttentionLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): SelfAttentionLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): SelfAttentionLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): SelfAttentionLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): SelfAttentionLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (transformer_cross_attention_layers): ModuleList(
          (0): CrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): CrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): CrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): CrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): CrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): CrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): CrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): CrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): CrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (transformer_ffn_layers): ModuleList(
          (0): FFNLayer(
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): FFNLayer(
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): FFNLayer(
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): FFNLayer(
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): FFNLayer(
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): FFNLayer(
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): FFNLayer(
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): FFNLayer(
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): FFNLayer(
            (linear1): Linear(in_features=512, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=512, bias=True)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (query_feat): Embedding(101, 512)
        (query_embed): Embedding(101, 512)
        (level_embed): Embedding(3, 512)
        (input_proj): ModuleList(
          (0): Sequential()
          (1): Sequential()
          (2): Sequential()
        )
        (lang_encoder): LanguageEncoder(
          (lang_encoder): Transformer(
            (token_embedding): Embedding(49408, 512)
            (resblocks): ModuleList(
              (0): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (1): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (2): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (3): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (4): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (5): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (6): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (7): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (8): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (9): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (10): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
              (11): ResidualAttentionBlock(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (ln_1): LayerNorm()
                (mlp): Sequential(
                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)
                  (gelu): QuickGELU()
                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)
                )
                (ln_2): LayerNorm()
                (drop_path): Identity()
              )
            )
            (ln_final): LayerNorm()
          )
        )
        (mask_embed): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
          )
        )
        (pos_embed_caping): Embedding(77, 512)
      )
    )
  )
)